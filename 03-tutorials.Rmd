# Tutorials 

```{r include=FALSE}
library(tidyverse)
library(rvest)
```


## Writing functions in R 

TBD


## Working with different datatypes (and how to convert between them)

TBD


## Regular expressions in R 

Like Zach, I'm not good at Regex so I'm just going to add examples that I previously used in my projects. See this [cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf) or this [cheatsheet](https://github.com/rstudio/cheatsheets/blob/main/regex.pdf) for some useful guidance I wish I knew. 

### Punctuations 

Let's filter for titles that contain any type of punctuation. This code gives 5 titles with punctuation: 

```{r echo=FALSE}
psych_df <- read.csv("data/psychologystudents_20240820.csv")
```

```{r}
psych_df %>%
  mutate(
    punct = str_detect(title, regex("[:punct:]+"))
  ) %>%
  filter(punct == TRUE) %>%
  select(title) %>%
  head(5)
```

The "+" after [:punct:] will match punctuation at least once, so multiple punctuation in a row will be detected too e.g. "I<b>....</b>want to score well in DAPR3" 

This code gives titles without any punctuation: 

```{r}
psych_df %>%
  mutate(
    punct = str_detect(title, regex("[:punct:]+"))
  ) %>%
  filter(punct == FALSE) %>%
  select(title) %>%
  head(5)
```

We can also use regex to separate strings into smaller ones by punctuation of any sort. 

```{r}
basic_string <- "I, kinda really - want to \ do well in BIG! Data! WOOO"

str_split(basic_string, regex("[:punct:]+")) %>%
  as.data.frame(col.names = "words") %>%
  mutate(
    words =   str_trim(words, side = "both")
  )
```

str_trim gets rid of white space to the left, right, or both sides of the string. 

### Fully capitalised words 

Now lets try to count the number of fully capitalised words in a title. I staggered the counts to see what each part of the regex does. 

```{r}
psych_df %>%
  mutate(
    fullycapitalised1 = str_count(title, regex("[A-Z]")),
    fullycapitalised2 = str_count(title, regex("[A-Z]{2,}")), 
    fullycapitalised3 = str_count(title, regex("\\b[A-Z]{2,}\\b"))
  ) %>%
  arrange(desc(fullycapitalised3)) %>%
  select(title, fullycapitalised1, fullycapitalised2, fullycapitalised3) %>%
  head(5)
```

- The first regex code counts the number of upper case letters in the title. So for the first title there are U, B, C... 10 capital letters. 
- The second regex code captures words with two or more consecutive capitalised letters. 
- The third regex code adds white space (empty string) before and after the capture word so that words like "MEd" or "MSc" don't get counted.

### Negated words 

```{r}
psych_df %>%
  mutate(
    n_negations = str_count(body, regex("n't"))
  ) %>%
  select(body, n_negations) %>%
  filter(n_negations == 2)
```



More examples tbd. 

## Scraping data from HTML 

This tutorial is adapted from [Chris Bail's Screen-scraping in R](https://cbail.github.io/textasdata/screenscraping/rmarkdown/Screenscraping_in_R.html) & [Wickham et al.'s R for Data Science](https://r4ds.hadley.nz/webscraping). 

We'll be using the package [rvest](https://rvest.tidyverse.org/index.html) to scrape information from html pages, which is useful for information contained within tables such as this [Interjection dictionary](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html) or the top movies on [IMDB](https://www.imdb.com/chart/top/).

Let's start with the table in [Interjection dictionary](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html). 

First, we'll read the entire html source code from the website we are interest into R. 

```{r}
interjections_html <- read_html("https://www.vidarholen.net/contents/interjections/") # Reading the entire page into R 
interjections_html
```

The result is HTML code, which is the programming language web developers use to define the structure and content of the website. In Google, you can inspect the html code of any website by right clicking with your cursor and pressing the "inspect" tab. Importantly, HTML code has a nested structure, typically including "head" a "body" sections of the webpage. The content are mostly contained within tags such as "p" (paragraph) and "h1" (heading 1), I won't go into too much detail what these are but feel free to look up any HTML tutorial to learn more about these elements.

```{r echo=FALSE, out.width = '800px', fig.align='center'}
knitr::include_graphics('./assets/html_inspect_screenshot.png')
```

Once you have the html code in R, you can use the html_nodes() function to specify what sort of elements you want to extract. For our purpose, we will specify "table":  

```{r}
interjections_dict1 <- interjections_html %>% 
  html_nodes("table") %>%
  .[[1]] %>% # Which table you want to extract (if there are multiple tables on a webpage)
  html_table()

head(interjections_dict1) # This gives you a dataframe that you can work with! 
```

Alternatively, you can specify an xpath to directly call the specific element you want to turn into a dataframe. You can find the xpath of an element by hovering over the HTML code that highlights the part of the website you want. For instance: 

```{r echo=FALSE, out.width = '800px', fig.align='center'}
knitr::include_graphics('./assets/html_xpath_screenshot.png')
```

Learn more about xpaths [here](https://www.w3schools.com/xml/xpath_syntax.asp).

```{r}
interjections_dict2 <- html_node(interjections_html, xpath = '//*[@id="it"]') %>% 
  html_table()
head(interjections_dict2)
```

### What if you wanted other information contained within a page? 

You can scrape specific elements of a webpage, depending on the HTML element you specify. Let's say you want to scrape all the text within paragraphs ("p") of [Sigmund Freud's Wikipedia](https://en.wikipedia.org/wiki/Sigmund_Freud) website. 

```{r}
freud_html <- read_html("https://en.wikipedia.org/wiki/Sigmund_Freud") %>% # Reading the entire wiki page into R 
  html_elements("p") %>% # Specify all "p" elements in the webpage
  html_text2() # Extract the text content of the HTML elements 

freud_html[[3]] # This gives you a list of all the paragraph text on the webpage
```

### Some caveats 

<b> Legality </b>

If the data is public, non-personal, and factual, it should be fine to scrape*. You should cite the websites you use, and always read the terms of conditions if present. 

<b> APIs </b> 

This short tutorial doesn't cover more dynamic websites, or websites that utilise an Application Programming Interface (API), which are a set of functions or procedures the website has specified to access their data. 
